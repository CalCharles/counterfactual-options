# python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 15 --graph-dir data/robopushing/block_graph --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type negparam --parameterized-lambda 10 --true-reward-lambda -2 --reward-constant -0.2 --epsilon-close .05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 150 --train --hidden-sizes 128 256 512 1024 128 --learning-type herddpg --grad-epoch 300 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .2 --gpu 1 --resample-timer -1 --tau .001 --log-interval 50 --max-steps 150 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .15 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --interleave --terminate-reset --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/reward_ra15 --log-only --save-graph /hdd/robopushing/live_record/graphs/reward_graph_ra15 --save-interval 100 > logs/robopushing/reward_tuning/train_reward_ra15.txt &

# python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 15 --graph-dir data/robopushing/block_graph --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type negparam --parameterized-lambda 10 --true-reward-lambda -2 --reward-constant -0.2 --epsilon-close .05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 150 --train --hidden-sizes 128 256 512 1024 128 --learning-type herddpg --grad-epoch 300 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .2 --gpu 1 --resample-timer -1 --tau .001 --log-interval 50 --max-steps 150 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .1 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --interleave --terminate-reset --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/reward_ra10 --log-only --save-graph /hdd/robopushing/live_record/graphs/reward_graph_ra10 --save-interval 100 > logs/robopushing/reward_tuning/train_reward_ra10.txt 

# python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 15 --graph-dir data/robopushing/block_graph --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type negparam --parameterized-lambda 10 --true-reward-lambda -2 --reward-constant -0.2 --epsilon-close .05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 150 --train --hidden-sizes 128 256 512 1024 128 --learning-type herddpg --grad-epoch 300 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .2 --gpu 2 --resample-timer -1 --tau .001 --log-interval 50 --max-steps 150 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .05 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --interleave --terminate-reset --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/reward_ra05 --log-only --save-graph /hdd/robopushing/live_record/graphs/reward_graph_ra05 --save-interval 100 > logs/robopushing/reward_tuning/train_reward_ra05.txt &

# python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 15 --graph-dir data/robopushing/block_graph --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 1000 --gamma 0.9 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type negparam --parameterized-lambda 10 --true-reward-lambda -2 --reward-constant -0.2 --epsilon-close .05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 150 --train --hidden-sizes 128 256 512 1024 128 --learning-type herddpg --grad-epoch 300 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .2 --gpu 2 --resample-timer -1 --tau .001 --log-interval 50 --max-steps 150 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .1 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --interleave --terminate-reset --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/rewardg90 --log-only --save-graph /hdd/robopushing/live_record/graphs/reward_graphg90 --save-interval 100 > logs/robopushing/reward_tuning/train_rewardg90.txt &

# python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 15 --graph-dir data/robopushing/block_graph --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type negparam --parameterized-lambda 10 --true-reward-lambda -2 --reward-constant -1 --epsilon-close .05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 150 --train --hidden-sizes 128 256 512 1024 128 --learning-type herddpg --grad-epoch 300 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .2 --gpu 3 --resample-timer -1 --tau .001 --log-interval 50 --max-steps 150 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .1 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --interleave --terminate-reset --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/rewardrc10 --log-only --save-graph /hdd/robopushing/live_record/graphs/reward_graphrc10 --save-interval 100 > logs/robopushing/reward_tuning/train_rewardrc10.txt 

# python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 15 --graph-dir data/robopushing/block_graph --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type negparam --parameterized-lambda 10 --true-reward-lambda -2 --reward-constant -0.2 --epsilon-close .05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 150 --train --hidden-sizes 128 256 512 1024 128 --learning-type hersac --grad-epoch 300 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .2 --gpu 3 --resample-timer -1 --tau .001 --log-interval 50 --max-steps 150 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .1 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --interleave --terminate-reset --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/rewardsac --log-only --save-graph /hdd/robopushing/live_record/graphs/reward_graphsac --save-interval 100 > logs/robopushing/reward_tuning/train_rewardsac.txt

# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 3000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 3000 --train --hidden-sizes 128 128 256 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 5 --test-trials 5 --temporal-extend 300 --interaction-prediction 0 --breakout-variant breakout_priority_large --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --test-episode --max-critic 100 --log-interval 25 --save-graph data/breakout/breakout_priority_large1 --save-interval 100 > logs/breakout/variant_trials/prio/train_breakout_priority_large1.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 3000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 3000 --train --hidden-sizes 128 128 256 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 5 --test-trials 5 --temporal-extend 300 --interaction-prediction 0 --breakout-variant breakout_priority_large --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --test-episode --max-critic 100 --log-interval 25 --save-graph data/breakout/breakout_priority_large2 --save-interval 100 > logs/breakout/variant_trials/prio/train_breakout_priority_large2.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 3000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 3000 --train --hidden-sizes 128 128 256 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 5 --test-trials 5 --temporal-extend 300 --interaction-prediction 0 --breakout-variant breakout_priority_large --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --test-episode --max-critic 100 --log-interval 25 --save-graph data/breakout/breakout_priority_large3 --save-interval 100 > logs/breakout/variant_trials/prio/train_breakout_priority_large3.txt

python3 train_offline_baseline.py --algorithm rainbow --observation-type multi-block-encoding --variant big_block --seed 456 --epoch 50 --video-log-period 5 --step-per-epoch 100000 --no-render --resume-path data/policy.pth --device cuda:1
python3 train_offline_baseline.py --algorithm rainbow --observation-type multi-block-encoding --variant big_block --seed 567 --epoch 50 --video-log-period 5 --step-per-epoch 100000 --no-render --resume-path data/policy.pth --device cuda:1