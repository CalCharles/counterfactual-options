python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/harden_single --save-interval 100 > logs/breakout/train_harden_single.txt
