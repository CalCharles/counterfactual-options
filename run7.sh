# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single1 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single1.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single2 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single2.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single3 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single3.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single4 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single4.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single5 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single5.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single6 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single6.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single7 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single7.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single8 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single8.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single9 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single9.txt
# python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 1000 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 --learning-type rainbow --grad-epoch 200 --pretrain-iters 10000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --terminate-cutoff --max-critic 100  --test-episode --log-interval 25 --save-graph data/breakout/variant_trials/hard_single/harden_single10 --save-interval 100 > logs/breakout/variant_trials/hard_single/train_harden_single10.txt

python3 train_offline_baseline.py --algorithm rainbow --observation-type multi-block-encoding --variant center_large --seed 567 --epoch 50 --video-log-period 5 --step-per-epoch 100000 --no-render --resume-path data/policy.pth --device cuda:1