requirements: pytorch, opencv, imageio, tianshou, robosuite

# Installation creating conda environment 
conda create -n rbt python=3.8
conda activate rbt
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
pip install tianshou / pip install git+https://github.com/thu-ml/tianshou.git@master --upgrade
conda install imageio
pip install opencv-python
conda install psutil


# Installing Robosuite with pushing domain (from source)
git clone https://github.com/kvablack/robosuite.git
cd robosuite
pip install -r requirements.txt
https://robosuite.ai/docs/installation.html
conda activate rbt
copy mujoco download to: ~/.mujoco/mujoco200
copy mujoco key to ~/.mujoco/mjkey.txt


python add_option.py --object Raw --option-type raw --true-environment --env Pend-Gym --buffer-steps 500000 --num-steps 1 --gamma .99 --batch-size 128 --num-iters 20000 --terminal-type true --reward-type true --epsilon-close 1 --init-form none --train --normalize --policy-type actorcritic --learning-type ddpg --grad-epoch 5 --warm-up 128 --warm-update 0 --lr 1e-4 --epsilon 0 --behavior-type greedyQ --return-form none --Q-critic --gpu 2 --log-interval 200 --optim Adam --factor 8 --num-layers 2 --use-layer-norm --double-Q .001 --actor-critic-optimizer

python add_option.py --object Raw --option-type raw --true-environment --env Nav2D --buffer-steps 500000 --num-steps 50 --gamma .99 --batch-size 16 --num-iters 4000 --terminal-type true --reward-type true --epsilon-close 1 --time-cutoff 50 --policy-type grid --learning-type herdqn --grad-epoch 50 --warm-up 10000 --lr 1e-4 --epsilon .1 --return-form none --select-positive .5 --gpu 2 --epsilon-schedule 250 --log-interval 25 --resample-timer 50 --factor 16--tau 60


Gym tests:
# sac
python train_option.py --gpu 1 --hidden-sizes 128 128 128 --env gymenvPendulum-v0 --learning-type sac --actor-lr 1e-4 --critic-lr 1e-3 --tau .005 --num-steps 10 --true-environment --option-type raw --pretrain-iters 1000 --num-iters 7000 --buffer-len 100000

# ddpg
python train_option.py --gpu 1 --hidden-sizes 128 128 128 --env gymenvPendulum-v0 --learning-type ddpg --actor-lr 1e-4 --critic-lr 1e-3 --tau .005 --num-steps 10 --true-environment --option-type raw --pretrain-iters 1000 --num-iters 7000 --buffer-len 100000

# dqn
python train_option.py --gpu 1 --hidden-sizes 128 128 128 --env gymenvCartPole-v0 --learning-type dqn --actor-lr 1e-4 --critic-lr 1e-3 --tau 100 --num-steps 10 --true-environment --option-type raw --pretrain-iters 1000 --num-iters 2000 --buffer-len 50000

# ppo
python train_option.py --gpu 1 --hidden-sizes 64 64 --env gymenvCartPole-v0 --learning-type ppo --actor-lr 1e-3 --critic-lr 1e-3 --tau 100 --num-steps 64 --true-environment --option-type raw --pretrain-iters 1000 --num-iters 20000 --buffer-len 20000 --grad-epoch 48 --log-interval 25

python train_option.py --gpu 1 --hidden-sizes 64 64 --env gymenvCartPole-v0 --learning-type ppo --actor-lr 1e-3 --num-steps 64 --true-environment --option-type raw --pretrain-iters 1000 --num-iters 20000 --buffer-len 20000 --grad-epoch 48 --log-interval 25

python train_option.py --gpu 1 --hidden-sizes 128 128 128 --env gymenvPendulum-v0 --learning-type ppo --actor-lr 1e-3 --num-steps 200 --true-environment --option-type raw --buffer-len 20000 --num-iters 2000 --log-interval 10 --grad-epoch 125


Nav2D grid test
# DQN with HER
python train_option.py --object Raw --option-type raw --true-environment --env Nav2D --buffer-len 1000000 --num-steps 50 --gamma .99 --batch-size 16 --num-iters 1500 --terminal-type env --reward-type env  --epsilon-close 1 --time-cutoff 50 --policy-type grid --learning-type herdqn --grad-epoch 50 --pretrain-iters 20000 --lr 1e-4 --epsilon .1 --select-positive .5 --gpu 2 --epsilon-schedule 2000 --log-interval 1 --max-steps 50 --hidden-sizes 32 64 64 564 --tau 3000 --resample-timer 50 --test-trials 3


Running Tianshou
# generate random data
python generate_breakout_data.py data/breakout/random 3000

# train the paddle motion model
python construct_hypothesis_model.py --record-rollouts data/breakout/random/ --env SelfBreakout --log-interval 500 --hidden-sizes 1024 1024 --predict-dynamics --action-shift --batch-size 10 --pretrain-iters 10000 --epsilon-schedule 3000 --num-iters 20000 --interaction-binary -1 -8 -16 --train --interaction-prediction .3 --gpu 2 --save-dir data/breakout/interaction_ap > logs/breakout/action_paddle_interaction.txt

# Run "test" to fill in parameter values for the hypothesis model
python construct_hypothesis_model.py --record-rollouts data/breakout/random/ --graph-dir data/breakout/action_graph/ --dataset-dir data/breakout/interaction_ap --env SelfBreakout --batch-size 64 --predict-dynamics --epsilon-schedule 1000 --action-shift --log-interval 1000 --num-frames 100000 --interaction-binary -1 -8 -16 --gpu 2 --save-dir data/breakout/interaction_apv > logs/breakout/action_paddle_interactionv.txt

# train the paddle policy with HER and DQN
python train_option.py --dataset-dir data/breakout/interaction_apv/ --object Paddle --option-type model --buffer-len 500000 --num-steps 75 --gamma .99 --batch-size 64 --num-iters 4000 --terminal-type comb --reward-type comb --sampler-type cuuni --parameterized-lambda 30 --epsilon-close 1 --set-time-cutoff  --hidden-sizes 128 128 --learning-type herdqn --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --epsilon-schedule 200 --select-positive .5 --gpu 2 --resample-timer 20 --tau 2500 --log-interval 100 --reward-constant -2 --time-cutoff 75 --max-steps 30 --print-test --use-termination --param-recycle .25 --sample-continuous 2 --sample-schedule 10000 --sample-distance 0.4 --pretest-trials 20 --hardcode-norm breakout 1 5 --interaction-probability 0 --interaction-prediction 0 --observation-setting 1 0 0 1 1 0 0 0 0 --drop-stopping --record-rollouts data/breakout/paddle --save-graph data/breakout/paddle_graph --save-interval 100 > logs/breakout/train_paddle.txt

python train_option.py --dataset-dir data/breakout/interaction_apv/ --object Paddle --option-type model --buffer-len 500000 --num-steps 75 --gamma .99 --batch-size 16 --num-iters 4000 --terminal-type comb --reward-type comb --sampler-type cuuni --parameterized-lambda 30 --epsilon-close 1 --set-time-cutoff  --hidden-sizes 128 128 --learning-type herdqn --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --epsilon-schedule 200 --select-positive .5 --gpu 2 --resample-timer 0 --tau 2500 --log-interval 100 --reward-constant -2 --time-cutoff 75 --max-steps 30 --print-test --use-termination --param-recycle .1 --sample-continuous 2 --sample-schedule 10000 --pretest-trials 20 --hardcode-norm breakout 1 5 --interaction-probability 0 --interaction-prediction 0 --observation-setting 1 0 0 1 1 0 0 0 0 > out.txt

# test the paddle policy
python test_option.py --dataset-dir data/breakout/interaction_apv/ --graph-dir data/breakout/paddle_graph --object Paddle --buffer-len 500000 --gamma .99 --test-trials 20 --num-iters 10 --terminal-type comb --reward-type comb --parameterized-lambda 0 --epsilon-close 1 --time-cutoff 50 --set-time-cutoff --gpu 2 --reward-constant -1 --max-steps 30 --print-test --sample-continuous 2 --param-recycle 0.1 --visualize-param nosave --record-rollouts data/paddle_test > logs/test_paddle.txt

# train the ball bounce interaction model
# note: alter feature_explorer.py ATM to speed up paddle-ball interaction
python construct_hypothesis_model.py --env SelfBreakout --record-rollouts data/breakout/paddle/ --graph-dir data/breakout/paddle_graph/ --hidden-sizes 512 512 --batch-size 64 --pretrain-iters 100000 --interaction-iters 200000 --epsilon-schedule 1000 --num-iters 200000 --log-interval 1000 --num-frames 300000 --interaction-binary -1 -13 -13 --train --gpu 2  --interaction-prediction .1 --train-pair Paddle Ball --save-dir data/breakout/interaction_pbI > logs/breakout/paddle_ball_interactionI.txt

# training paddle-ball interaction with proximity
python construct_hypothesis_model.py --env SelfBreakout --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/paddle/ --graph-dir data/breakout/paddle_graph/ --hidden-sizes 256 256 256 128 --batch-size 128 --pretrain-iters 50000 --epsilon-schedule 30000 --num-iters 100000 --log-interval 1000 --num-frames 300000 --interaction-binary -7 0 -3 --train --gpu 2  --interaction-prediction .5 --train-pair Paddle Ball --drop-stopping --compare-trace --passive-weighting 10 --inline-iters 5 --max-distance-epsilon 7 --passive-weight-interaction-iters 20000 --save-dir data/breakout/interaction_pbproxbinI > logs/breakout/paddle_ball_interactionproxbinI.txt

# training paddle-ball interaction with proximity and interaction boosting
python construct_hypothesis_model.py --env SelfBreakout --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/paddle/ --graph-dir data/breakout/paddle_graph/ --hidden-sizes 256 256 256 128 --batch-size 128 --pretrain-iters 50000 --epsilon-schedule 30000 --num-iters 100000 --log-interval 1000 --num-frames 300000 --interaction-binary -7 0 -3 --train --gpu 2  --interaction-prediction .5 --train-pair Paddle Ball --drop-stopping --compare-trace --passive-weighting 10 --inline-iters 5 --max-distance-epsilon 7 --passive-weight-interaction-iters 20000 --interaction-boosting .2 --save-dir data/breakout/interaction_pbproxibI > logs/breakout/paddle_ball_interactionproxibI.txt

python construct_hypothesis_model.py --env SelfBreakout --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/paddle/ --graph-dir data/breakout/paddle_graph/ --hidden-sizes 256 256 256 128 --batch-size 128 --lr 1e-3 --critic-lr 5e-4 --pretrain-iters 50000 --epsilon-schedule 30000 --num-iters 100000 --log-interval 1000 --num-frames 300000 --interaction-binary -7 0 -3 --train --gpu 2  --interaction-prediction .5 --train-pair Paddle Ball --drop-stopping --compare-trace --passive-weighting 10 --inline-iters 5 --max-distance-epsilon 7 --passive-weight-interaction-iters 20000 --interaction-boosting .2 --hardcode-norm SelfBreakout --save-dir data/breakout/interaction_pbproxlr3I > logs/breakout/paddle_ball_interactionproxlr3I.txt

python construct_hypothesis_model.py --env SelfBreakout --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/paddle/ --graph-dir data/breakout/paddle_graph/ --hidden-sizes 128 128 128 --batch-size 128 --lr 1e-3 --critic-lr 1e-3 --pretrain-iters 50000 --epsilon-schedule 5000 --num-iters 30000 --log-interval 1000 --num-frames 300000 --interaction-binary -6 0 -1 --train --gpu 2  --interaction-prediction .5 --train-pair Paddle Ball --drop-stopping --compare-trace --passive-weighting 10 --inline-iters 20 --max-distance-epsilon 7 --passive-weight-interaction-iters 20000 --interaction-boosting .2 --hardcode-norm SelfBreakout --save-dir data/breakout/interaction_pbproxI > logs/breakout/paddle_ball_interactionproxI.txt

# Run "test" to fill in parameter values for the hypothesis model
python construct_hypothesis_model.py --env SelfBreakout --record-rollouts data/breakout/paddle/ --graph-dir data/breakout/ball_graph/ --dataset-dir data/breakout/interaction_pbI --hidden-sizes 512 512 --batch-size 64 --epsilon-schedule 1000 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --gpu 2 --save-dir data/breakout/interaction_pbIv > logs/breakout/paddle_ball_interactionIv.txt

python construct_hypothesis_model.py --env SelfBreakout --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/paddle/ --graph-dir data/breakout/ball_graph/ --dataset-dir data/breakout/interaction_pbproxI --batch-size 64 --epsilon-schedule 1000 --log-interval 1000 --num-frames 300000 --interaction-binary -6 0 -1 --gpu 2 --save-dir data/breakout/interaction_pbproxIv > logs/breakout/paddle_ball_interactionproxIv.txt


# train ball bounce interaction with relative state
python construct_hypothesis_model.py --env SelfBreakout --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/paddle --graph-dir data/breakout/paddle_graph/ --hidden-sizes 512 512 --batch-size 64 --pretrain-iters 100000 --interaction-iters 200000 --epsilon-schedule 1000 --num-iters 200000 --posttrain-iters 0 --log-interval 1000 --num-frames 300000 --interaction-binary -1 -13 -13 --train --gpu 2  --interaction-prediction .1 --train-pair  --observation-setting 1 --save-dir data/breakout/interaction_pbR > logs/breakout/paddle_ball_interactionR.txt

# test ball bounce interaction with relative state
python construct_hypothesis_model.py --env SelfBreakout --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/paddle --graph-dir data/breakout/ball_graph/ --dataset-dir data/breakout/interaction_pbR --hidden-sizes 512 512 --batch-size 64 --epsilon-schedule 1000 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --gpu 2 --save-dir data/breakout/interaction_pbRv > logs/breakout/paddle_ball_interactionRv.txt

# train ball bouncing with SAC-HER velocity
# 100, 200 reward
python train_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/paddle_graph --object Ball --option-type model --buffer-len 1000000 --num-steps 250 --gamma .99 --batch-size 128 --num-iters 20000 --terminal-type tcomb --reward-type tcomb --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 128 128 128 --learning-type herddpg --grad-epoch 50 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --select-positive .1 --tau .001 --log-interval 100 --reward-constant -1 --interaction-probability 1 --interaction-prediction 0 --max-steps 300 --relative-action 7 --pretest-trials 20 --prioritized-replay 0.2 0.4 --temporal-extend 3 --lookahead 5 --print-test --force-mask 0 0 1 1 0 --use-interact --max-hindsight 20 --param-interaction --use-termination --gpu 2 --hardcode-norm breakout 3 1 --her-only-interact 1 --resample-timer 0 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --env-reset --sample-merged --max-critic 200 --save-graph data/breakout/ball_graph --save-interval 100 --record-rollouts data/breakout/ball > logs/breakout/train_ball.txt

python train_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/paddle_graph --object Ball --option-type model --buffer-len 500 --num-steps 250 --gamma .99 --batch-size 16 --num-iters 10 --terminal-type tcomb --reward-type tcomb --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 128 128 128 --learning-type herddpg --grad-epoch 50 --pretrain-iters 1000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --select-positive .5 --tau .001 --log-interval 100 --reward-constant -1 --interaction-probability 1 --interaction-prediction 0 --max-steps 300 --relative-action 6 --pretest-trials 20 --prioritized-replay 0.2 0.4 --temporal-extend 3 --lookahead 4 --print-test --force-mask 0 0 1 1 0 --use-interact --max-hindsight 20 --param-interaction --use-termination --gpu 2 --hardcode-norm breakout 3 1 --her-only-interact 1 --resample-timer 0 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --env-reset > out.txt

# train ball bouncing with SAC-HER velocity
# 0, 1, -1 reward
python train_option.py --dataset-dir data/breakout/interaction_pbproxlr3Iv/ --graph-dir data/breakout/paddle_graph --object Ball --option-type model --buffer-len 1000000 --num-steps 250 --gamma .99 --batch-size 128 --num-iters 20000 --terminal-type tcomb --reward-type tcomb --interaction-lambda 0 --parameterized-lambda 1 --true-reward-lambda 1 --reward-constant 0 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 256 256 128 128 128 --learning-type herddpg --grad-epoch 50 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --select-positive .1 --tau .001 --log-interval 100 --interaction-probability 1 --interaction-prediction 0 --max-steps 300 --relative-action 6 --pretest-trials 20 --prioritized-replay 0.2 0.4 --temporal-extend 3 --lookahead 4 --print-test --force-mask 0 0 1 1 0 --use-interact --max-hindsight 20 --param-interaction --use-termination --gpu 2 --hardcode-norm breakout 3 1 --her-only-interact 1 --resample-timer 0 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --env-reset --sample-merged --max-critic 2 --save-graph data/breakout/ball_prox_graphmc2 --save-interval 100 --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/ball_proxmc2 > logs/breakout/train_ball_proxmc2.txt

python train_option.py --dataset-dir data/breakout/interaction_pbproxlr3Iv/ --graph-dir data/breakout/paddle_graph --object Ball --option-type model --buffer-len 1000000 --num-steps 250 --gamma .99 --batch-size 128 --num-iters 20000 --terminal-type tcomb --reward-type tcomb --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 128 128 128 --learning-type herddpg --grad-epoch 50 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.05 --behavior-type greedyQ --select-positive .1 --tau .001 --log-interval 100 --reward-constant -1 --interaction-probability 1 --interaction-prediction 0 --max-steps 300 --relative-action 7 --pretest-trials 20 --prioritized-replay 0.6 0.4 --temporal-extend 3 --lookahead 5 --print-test --force-mask 0 0 1 1 0 --use-interact --max-hindsight 20 --param-interaction --use-termination --gpu 2 --hardcode-norm breakout 3 1 --her-only-interact 1 --resample-timer 0 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --env-reset --sample-merged --max-critic 200 --save-graph /hdd/datasets/counterfactual_data/breakout/live_record/prox_tests/ball_prox_graph1 --save-interval 100 > logs/breakout/prox_tests/train_ball_prox1.txt




# Train refined ball forward model
python construct_hypothesis_model.py --env SelfBreakout --record-rollouts data/breakout/ball/ --graph-dir data/breakout/paddle_graph/ --hidden-sizes 512 512 --batch-size 64 --pretrain-iters 100000 --interaction-iters 100000 --epsilon-schedule 1000 --num-iters 200000 --posttrain-iters 0 --log-interval 1000 --num-frames 300000 --interaction-binary -1 -13 -13 --train --gpu 2  --interaction-prediction .1 --train-pair Paddle Ball --save-dir data/breakout/interaction_pbIr > logs/breakout/paddle_ball_interactionIr.txt

# Test refined ball bouncing
python construct_hypothesis_model.py --env SelfBreakout --record-rollouts data/breakout/ballt1/ --graph-dir data/breakout/ball_graph/ --dataset-dir data/breakout/interaction_pbIr --hidden-sizes 512 512 --batch-size 64 --epsilon-schedule 1000 --log-interval 1000 --num-frames 300000 --interaction-binary -1 -13 -13 --gpu 2 --save-dir data/breakout/interaction_pbIrv > logs/breakout/paddle_ball_interactionIrv.txt

# test SAC-HER velocity policy with model
python test_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/ball_graph --object Ball --buffer-len 500000 --gamma .99  --test-trials 120 --num-iters 10 --terminal-type tcomb --reward-type tcomb --option-type forward --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --sample-continuous 1 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --relative-action 6 --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 1 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --hardcode-norm breakout 3 1 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --change-option --gpu 3 --env-reset --record-rollouts data/breakout/ball_test --save-dir data/breakout/ball_model_option --save-graph data/breakout/ball_model_graph > logs/breakout/test_ball_model.txt

# Test SAC-HER velocity policy without model
python test_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/ball_graph --object Ball --buffer-len 500000 --gamma .99  --test-trials 150 --num-iters 100 --terminal-type tcomb --reward-type tcomb --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --sample-continuous 1 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --relative-action 6 --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 1 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --hardcode-norm breakout 3 1 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --gpu 3 --env-reset --record-rollouts data/breakout/ball_test > logs/breakout/test_ball.txt

# test SAC-HER velocity policy with small domain
python test_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/ball_graph --object Ball --buffer-len 500000 --gamma .99  --test-trials 100 --num-iters 250 --terminal-type tcomb --reward-type tcomb --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --sample-continuous 1 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --relative-action 6 --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 1 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --hardcode-norm breakout 3 1 --block-shape 2 10 4 1 15 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --gpu 3 --env-reset --record-rollouts /hdd/datasets/counterfactual-data/breakout/ball_test_small > logs/breakout/test_ball_small.txt

python test_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/ball_graph --object Ball --buffer-len 500000 --gamma .99  --test-trials 150 --num-iters 150 --terminal-type tcomb --reward-type tcomb --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --sample-continuous 1 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --relative-action 6 --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 1 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --hardcode-norm breakout 3 1 --block-shape 5 20 4 1 120 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --gpu 3 --env-reset --record-rollouts /hdd/datasets/counterfactual-data/breakout/ball_test_full > logs/breakout/test_ball_full.txt

python test_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/ball_graph --object Ball --buffer-len 500000 --gamma .99  --test-trials 150 --num-iters 150 --terminal-type tcomb --reward-type tcomb --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --sample-continuous 1 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --relative-action 6 --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 1 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --hardcode-norm breakout 3 1 --block-shape 5 20 4 0 0 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --gpu 3 --env-reset --record-rollouts /hdd/datasets/counterfactual-data/breakout/ball_test_true > logs/breakout/test_ball_true.txt


# Test SAC-HER velocity policy in big block domain
python test_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/ball_graph --object Ball --target-mode --buffer-len 500000 --gamma .99  --test-trials 120 --num-iters 10 --terminal-type tcomb --reward-type tcomb --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --sample-continuous 1 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --relative-action 6 --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 1 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --hardcode-norm breakout 3 1 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --gpu 3 --env-reset --record-rollouts data/breakout/ball_test_target_block > logs/breakout/test_ball_target.txt

# Test SAC-HER vel policy model in big block domain
python test_option.py --dataset-dir data/breakout/interaction_pbIv/ --graph-dir data/breakout/ball_graph --object Ball --target-mode --buffer-len 500000 --gamma .99  --test-trials 120 --num-iters 10 --terminal-type tcomb --reward-type tcomb --option-type forward --interaction-lambda 1 --parameterized-lambda 200 --true-reward-lambda 0 --sample-continuous 1 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --relative-action 6 --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 1 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --hardcode-norm breakout 3 1 --drop-stopping --observation-setting 1 0 0 1 0 1 0 0 0 --change-option --gpu 3 --env-reset --record-rollouts data/breakout/ball_test_target_block_model --save-dir data/breakout/ball_model_option --save-graph data/breakout/ball_model_target_graph > logs/breakout/test_ball_target_model.txt

<!-- # train block interaction model -->
<!-- python construct_hypothesis_model.py --env SelfBreakout --record-rollouts data/ball_test/ --hidden-sizes 512 512 --batch-size 64 --pretrain-iters 100000 --interaction-iters 100000 --epsilon-schedule 1000 --num-iters 200000 --posttrain-iters 0 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --graph-dir data/ball_vel_graphs/ --train --gpu 2  --interaction-prediction .3 --policy-type pair --interaction-weight 10 --save-dir data/interaction_bbI > logs/ball_block_interactionI.txt
 -->
<!-- python construct_hypothesis_model.py --env SelfBreakout --record-rollouts data/ball_test/ --hidden-sizes 512 512 --batch-size 64 --pretrain-iters 20000 --interaction-iters 30000 --epsilon-schedule 1000 --num-iters 30000 --posttrain-iters 0 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --graph-dir data/ball_vel_graphs/ --train  --interaction-prediction .3 --policy-type pair --interaction-weight 10 --predict-dynamics --gpu 2 --save-dir data/interaction_bbI > logs/ball_block_interactionI.txt
 -->
# train block interaction model
/hdd/datasets/counterfactual_data/breakout/live_record/ball/
python construct_hypothesis_model.py --record-rollouts data/breakout/ball/ --graph-dir data/breakout/ball_graph/ --env SelfBreakout --hidden-sizes 512 512 --batch-size 64 --pretrain-iters 10000 --interaction-iters 30000 --epsilon-schedule 1000 --num-iters 40000 --posttrain-iters 0 --log-interval 1000 --num-frames 300000 --interaction-binary -1 -13 -13 --train --gpu 2  --interaction-prediction .3 --policy-type pair --hardcode-norm SelfBreakout --interaction-weight 10 --predict-dynamics --multi-instanced --train-pair Ball Block --save-dir data/breakout/interaction_bbI > logs/breakout/ball_block_interactionI.txt

python construct_hypothesis_model.py --record-rollouts data/breakout/ball/ --graph-dir data/ball_graph/ --env SelfBreakout --hidden-sizes 512 512 --batch-size 64 --pretrain-iters 10 --interaction-iters 20000 --epsilon-schedule 1000 --num-iters 40000 --posttrain-iters 0 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --train --gpu 2  --interaction-prediction .3 --policy-type pair --interaction-weight 10 --predict-dynamics --multi-instanced --train-pair Ball Block --save-dir data/interaction_bbI > logs/ball_block_interactionI.txt

# test block interaction model
python construct_hypothesis_model.py --record-rollouts data/breakout/ball/ --graph-dir data/breakout/ball_graph --dataset-dir data/breakout/ball_block_interactionI --env SelfBreakout --hidden-sizes 512 512 --batch-size 64 --epsilon-schedule 1000 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --gpu 2 --save-dir data/breakout/interaction_bbIv > logs/ball_block_interactionIv.txt

<!-- python construct_hypothesis_model.py --env SelfBreakout --record-rollouts data/ball_test/ --hidden-sizes 512 512 --batch-size 64 --epsilon-schedule 1000 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --graph-dir data/ball_vel_graphs/ --gpu 2 --dataset-dir data/interaction_ballblock -->

# train block targeting policy
python train_option.py --dataset-dir data/interaction_bbIv/ --graph-dir data/vel_model_graph --object Block --option-type model --buffer-len 500000 --num-steps 500 --gamma .75 --batch-size 32 --num-iters 10000 --terminal-type proxist --reward-type proxist --interaction-lambda 0 --parameterized-lambda 200 --true-reward-lambda 100 --epsilon-close .5 --time-cutoff 500 --train --hidden-sizes 256 256 256 --learning-type herdqn --grad-epoch 250 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --sampler-type inst --select-positive .5 --gpu 2 --resample-timer 1 --tau .001 --log-interval 50 --reward-constant -1 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --sac-alpha .2 --lookahead 4 --print-test --use-interact --max-hindsight 12 --param-interaction --use-termination --param-first --max-distance-epsilon 6 --observation-setting 1 0 0 1 0 0 0 0 0 --record-rollouts data/block --save-graph data/block_graph_model --save-interval 100 > logs/train_block_model.txt

python train_option.py --dataset-dir data/interaction_bbIv/ --graph-dir data/vel_model_graph --object Block --option-type model --buffer-len 500000 --num-steps 500 --gamma .75 --batch-size 32 --num-iters 10000 --terminal-type proxist --reward-type proxist --interaction-lambda 0 --parameterized-lambda 200 --true-reward-lambda 100 --epsilon-close .5 --time-cutoff 500 --train  --hidden-sizes 256 256 256 --learning-type hersac --grad-epoch 250 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --sampler-type inst --select-positive .5 --gpu 2 --resample-timer 1 --tau .001 --log-interval 50 --reward-constant -1 --interaction-probability 0 --max-steps 200 --pretest-trials 20 --temporal-extend 300 --sac-alpha .2 --lookahead 4 --print-test --use-interact --max-hindsight 12 --param-interaction --use-termination --param-first --max-distance-epsilon 5 --record-rollouts data/block_sac --save-graph data/block_graph_model_sac --save-interval 100 > logs/train_block_model_sac.txt

# Train true environment with angle policy and block termination
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type block --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type true --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 512 512 512 512 --learning-type dqn --grad-epoch 150 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --gpu 2 --tau .001 --log-interval 50 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-probability 0 --interaction-prediction 0 --only-termination --block-shape 2 10 4 -- --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --init-form xnorm --record-rollouts data/breakout/multi_block --save-graph data/breakout/multi_block_graph --save-interval 100 > logs/breakout/train_multi_block.txt


# Train dummy model miniblock policy
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type block --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type paramist --interaction-lambda 0 --parameterized-lambda 11 --reward-constant -1 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 512 512 512 512 --learning-type herdqn --grad-epoch 150 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --gpu 2 --tau .001 --log-interval 50 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-probability 0 --interaction-prediction 0 --use-termination --block-shape 1 10 4 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --init-form xnorm --record-rollouts data/breakout/multi_block --save-graph data/breakout/multi_block_graph --save-interval 100 > logs/breakout/train_multi_block.txt

python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type block --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type paramist --interaction-lambda 0 --parameterized-lambda 11 --reward-constant -1 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 512 512 512 512 --learning-type herdqn --grad-epoch 150 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --gpu 2 --tau .001 --log-interval 50 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-probability 0 --interaction-prediction 0 --use-termination --block-shape 2 10 4 0 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --init-form xnorm --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/multi_block --save-graph data/breakout/multi_block_pair_graph --save-interval 100 > logs/breakout/train_multi_block_pair.txt


python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type block --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type paramist --interaction-lambda 0 --parameterized-lambda 11 --reward-constant -1 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 512 512 512 512 --learning-type herdqn --grad-epoch 150 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --gpu 2 --tau .001 --log-interval 50 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-probability 0 --interaction-prediction 0 --use-termination --block-shape 1 10 4 0 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --record-rollouts data/breakout/multi_block --save-graph data/breakout/multi_block_graph --save-interval 100 > logs/breakout/train_multi_block.txt

# Train dummy model miniblock policy with no breakout
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type block --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type paramist --reward-constant -1 --parameterized-lambda 2 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 256 1024 128 --learning-type herdqn --grad-epoch 500 --pretrain-iters 1000000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --param-contained --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_blockdqn2.10.4.1.15/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/multi_block_no_breakout_dqn --save-graph data/breakout/multi_block_no_breakout_dqn_graph --save-interval 100 > logs/breakout/train_multi_block_no_breakout_dqn.txt

# Train dummy model miniblock policy with no breakout and proximity
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type randblock --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type proxist --reward-constant -1 --parameterized-lambda 2 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 256 1024 128 --learning-type herdqn --grad-epoch 500 --pretrain-iters 1000000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --param-contained --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_blockdqn_prox2.10.4.1.15/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/multi_block_no_breakout_dqn_prox --save-graph data/breakout/multi_block_no_breakout_dqn_prox_graph --save-interval 100 > logs/breakout/train_multi_block_no_breakout_dqn_prox.txt



# Train dummy model medium block policy with breakout and proximity
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type randblock --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 20000 --terminal-type inter --reward-type dist --reward-constant -1 --parameterized-lambda 2 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 256 1024 128 --learning-type herdqn --grad-epoch 500 --pretrain-iters 1000000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --select-positive .3 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --use-termination --block-shape 3 15 4 0 0 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --param-contained --max-distance-epsilon 12 --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_blockdqn_prox3.15.4.0.0/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/medium_block_no_breakout_dqn_prox --save-graph data/breakout/medium_block_no_breakout_dqn_prox_graph --save-interval 100 > logs/breakout/train_medium_block_no_breakout_dqn_prox.txt


# Train dummy model miniblock policy with no breakout with supervised iterated learning
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type block --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type paramist --reward-constant 0 --parameterized-lambda 1 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 128 128 --learning-type herisl --grad-epoch 150 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --gpu 3 --log-interval 100 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test --interaction-prediction 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --keep-proximity --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/multi_block_no_breakout_isl_prox --save-graph data/breakout/multi_block_no_breakout_isl_prox_graph --save-interval 100 > logs/breakout/train_multi_block_no_breakout_isl_prox.txt

python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type block --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type paramist --reward-constant 0 --parameterized-lambda 1 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 256 1024 128 --learning-type herisl --grad-epoch 500 --pretrain-iters 1000000 --lr 1e-5 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --param-contained --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_block2.10.4.1.15/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/multi_block_no_breakout_isl --save-graph data/breakout/multi_block_no_breakout_isl_graph --save-interval 100 > logs/breakout/train_multi_block_no_breakout_isl.txt

# save loaded network for dummy model miniblock policy
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type block --policy-type pair --terminal-type tparam --reward-type tconst --reward-constant 0 --parameterized-lambda 1 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 128 128 --interaction-probability 0 --temporal-extend 300 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --param-contained --save-graph data/breakout/loaded_block_policy --save-loaded-network --save-interval 100 --load-network data/breakout/network_test/net_ball_test_small.pt

# test loaded network for dummy model miniblock policy
python test_option.py --dataset-dir dummy --graph-dir data/breakout/loaded_block_policy --object Block --option-type model --sampler-type block --policy-type pair --terminal-type instinter --reward-type tconst --test-trials 40 --num-iters 10 --reward-constant 0 --parameterized-lambda 1 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 128 128 128 128 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 100 --param-contained --save-interval 100 --save-raw --record-rollouts data/breakout/test_loaded_miniblock > logs/breakout/test_loaded_miniblock.txt


# test dummy model miniblock policy (isl, replace with dqn)
python test_option.py --dataset-dir dummy --graph-dir data/breakout/multi_block_no_breakout_isl_graph --object Block --option-type model --sampler-type block --terminal-type inter --reward-type paramist --test-trials 15 --num-iters 300 --reward-constant 0 --parameterized-lambda 1 --time-cutoff 300 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 300 --only-termination --param-contained --save-interval 100 --save-raw --record-rollouts data/breakout/test_loaded_miniblock > logs/breakout/test_loaded_miniblock.txt

# test dummy model miniblock policy (dqn)
python test_option.py --dataset-dir dummy --graph-dir data/breakout/multi_block_no_breakout_dqn_prox_graph --object Block --option-type model --sampler-type randblock --terminal-type inter --reward-type paramist --test-trials 15 --num-iters 300 --reward-constant 0 --parameterized-lambda 1 --time-cutoff 300 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 300 --only-termination --param-contained --save-interval 100 --save-raw --record-rollouts data/breakout/test_loaded_miniblock_prox > logs/breakout/test_loaded_miniblock_prox.txt

# assess angle bouncing against trained option bouncing
python test_option.py --dataset-dir dummy --graph-dir data/breakout/multi_block_no_breakout_isl_graph --object Block --option-type model --sampler-type block --policy-type Assess --terminal-type inter --reward-type paramist --test-trials 0 --num-iters 300 --reward-constant 0 --parameterized-lambda 1 --time-cutoff 300 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --use-termination --block-shape 2 10 4 1 15 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 300 --only-termination --param-contained

# test target mode block policy
python test_option.py --dataset-dir dummy --graph-dir data/breakout/big_block_graph --object Block --target-mode --buffer-len 500000 --gamma .99  --test-trials 40 --num-iters 10 --terminal-type tparam --reward-type tconst --interaction-lambda 0 --parameterized-lambda 0 --true-reward-lambda 11 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 0 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --drop-stopping --observation-setting 1 0 0 1 0 0 0 0 0 --gpu 3 --env-reset --save-raw --record-rollouts data/breakout/test_big_block > logs/breakout/test_big_block.txt

# use HER with instance retargeting
python train_option.py --dataset-dir data/breakout/interaction_bbI/ --graph-dir data/breakout/ball_graph --object Block --option-type model --buffer-len 500000 --num-steps 500 --gamma 0 --batch-size 32 --num-iters 10000 --terminal-type param --reward-type param --interaction-lambda 0 --parameterized-lambda 1 --true-reward-lambda 0 --reward-constant -1 --epsilon-close .5 --time-cutoff 1 --train --hidden-sizes 256 256 256 --learning-type dqnher --grad-epoch 50 --pretrain-iters 10000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --sampler-type hstinst --select-positive .5 --gpu 2 --resample-timer 1 --tau .001 --log-interval 50 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-probability 0 --interaction-prediction 0 --use-termination --target-mode --observation-setting 1 0 0 1 0 0 0 0 0 --record-rollouts data/breakout/big_block --save-graph data/breakout/big_block_graph --save-interval 100 > logs/train_big_block_model.txt

python train_option.py --dataset-dir data/breakout/interaction_bbI/ --graph-dir data/breakout/ball_graph --object Block --option-type model --buffer-len 50 --num-steps 500 --gamma 0 --batch-size 8 --num-iters 5 --terminal-type param --reward-type param --interaction-lambda 0 --parameterized-lambda 10 --true-reward-lambda 0 --reward-constant -1 --epsilon-close .5 --time-cutoff 1000 --train --hidden-sizes 256 256 256 --learning-type dqn --grad-epoch 50 --pretrain-iters 100 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --sampler-type hstinst --gpu 2 --tau .001 --log-interval 50 --interaction-probability 0 --max-steps 150 --pretest-trials 1 --test-trials 1 --temporal-extend 300 --print-test  --interaction-probability 0 --interaction-prediction 0 --use-termination --target-mode --observation-setting 1 0 0 1 0 0 0 0 0

# test block targeting policy
python test_option.py --dataset-dir data/newrun/interaction_bbIv/ --graph-dir data/newrun/block_graph_model --object Block --buffer-len 500000 --gamma .99  --test-trials 10 --num-iters 5  --terminal-type proxist --reward-type proxist --sampler-type inst --option-type model --interaction-lambda 0 --parameterized-lambda 200 --true-reward-lambda 100 --epsilon-close .5 --time-cutoff 200 --temporal-extend 300 --reward-constant -1 --max-steps 5000 --print-test --use-interact --max-hindsight 12 --param-interaction --use-termination --param-contained --change-option --gpu 3 --max-distance-epsilon 5 --visualize nosave

# Train dummy model full block policy with no breakout and proximity ***
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Block --option-type model --sampler-type randblock --policy-type pair --buffer-len 100000 --num-steps 3000 --gamma 0.9 --batch-size 128 --num-iters 1000 --terminal-type inter --reward-type dist --reward-constant -1 --parameterized-lambda 2 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 256 256 512 1024 128 --learning-type hersac --grad-epoch 1000 --pretrain-iters 20000 --lr 5e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --use-termination --block-shape 5 20 4 1 100 --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --param-contained --max-distance-epsilon 10 --breakout-variant proximity --log-interval 25 --print-test --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/full_block_no_breakout_dqn_prox --save-graph data/breakout/full_block_no_breakout_dqn_prox_graph --save-interval 100 > logs/breakout/train_full_block_no_breakout_dqn_prox.txt

# train big block (target mode) block policy ***
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 1000 --terminal-type tparam --reward-type tconst --interaction-lambda 0 --parameterized-lambda 0 --true-reward-lambda 2 --reward-constant 0 --epsilon-close .5 --time-cutoff 300 --train --hidden-sizes 512 512 512 128 --learning-type sac --grad-epoch 600 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type hstinst --gpu 2 --tau .001 --log-interval 50 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-probability 0 --interaction-prediction 0 --use-termination --target-mode --breakout-variant big_block --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --terminate-cutoff --log-interval 25 --max-critic 10 --record-rollouts /hdd/datasets/counterfacutal_data/Breakout/live_record/big_block --save-graph data/breakout/big_block_graph --save-interval 100 > logs/breakout/train_big_block.txt

# Test big block (target mode) block policy ***
python test_option.py --dataset-dir dummy --graph-dir data/breakout/big_block_graph --object Block --target-mode --breakout-variant big_block --buffer-len 500000 --gamma .99  --test-trials 40 --num-iters 10 --terminal-type tparam --reward-type tconst --interaction-lambda 0 --parameterized-lambda 0 --true-reward-lambda 11 --epsilon-close .5 --time-cutoff -1 --set-time-cutoff --reward-constant -1 --max-steps 10000 --print-test --interaction-probability 0 --interaction-prediction 0 --force-mask 0 0 1 1 0 --use-interact --use-termination --param-interaction --drop-stopping --observation-setting 1 0 0 1 0 0 0 0 0 --gpu 3 --env-reset --save-raw --record-rollouts data/breakout/test_big_block > logs/breakout/test_big_block.txt

# Train negative reward policy with single row with ball graph ***
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 500 --gamma 0.9 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type tscale --reward-constant 0 --parameterized-lambda 1 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 128 128 256 512 128 --learning-type sac --grad-epoch 200 --pretrain-iters 30000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 0 --log-interval 100 --interaction-probability 0 --max-steps 2000 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --breakout-variant negative_rand_row --observation-setting 1 0 0 0 0 0 0 0 0 --prioritized-replay 0.4 0.2 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --test-episode --drop-stopping --max-critic 100 --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_negative_rand_row/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/negative_rand_row --save-graph data/breakout/negative_rand_row --save-interval 100 > logs/breakout/train_negative_rand_row.txt

# Test Negative block policy ***
python test_option.py --dataset-dir dummy --graph-dir data/breakout/negative_rand_row --object Block --option-type model --sampler-type randblock --terminal-type inter --reward-type tscale --test-trials 10 --num-iters 300 --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --time-cutoff 300 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --use-termination --breakout-variant negative_rand_row --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 2000 --only-termination --use-interact --test-episode --param-contained --drop-stopping --save-interval 100 --save-raw --record-rollouts data/breakout/test_negative_rand_row > logs/breakout/test_negative_rand_row.txt

# Train single block policy with ball graph ***
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 128 128 256 1024 128 --learning-type dqn --grad-epoch 50 --pretrain-iters 10000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 1 --log-interval 100 --interaction-probability 0 --max-steps 150 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --breakout-variant single_block --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --terminate-cutoff --test-episode --max-critic 100 --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_single_block/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/single_block --save-graph data/breakout/single_block --save-interval 100 > logs/breakout/train_single_block.txt

# Test single block policy ***
python test_option.py --dataset-dir dummy --graph-dir data/breakout/single_block --object Block --option-type model --sampler-type randblock --terminal-type inter --reward-type tscale --test-trials 10 --num-iters 300 --reward-constant -1 --true-reward-lambda 1 --parameterized-lambda 1 --time-cutoff 300 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --use-termination --breakout-variant single_block --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 2000 --only-termination --use-interact --test-episode --param-contained --visualize-param noparam/hdd/datasets/counterfactual_data/breakout/videos/single_block/ --save-interval 100 --save-raw --record-rollouts data/breakout/test_single_block > logs/breakout/test_single_block.txt

# Train breakout policy with ball graph ***
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 3000 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 4000 --train --hidden-sizes 128 128 256 1024 128 --learning-type dqn --grad-epoch 50 --pretrain-iters 100000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 4000 --pretest-trials 20 --test-trials 5 --temporal-extend 300 --print-test  --interaction-prediction 0 --breakout-variant breakout_priority_full --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --drop-stopping --test-episode --max-critic 100 --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_breakout_priority_full/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/breakout_priority_full --save-graph data/breakout/breakout_priority_full --save-interval 100 > logs/breakout/train_breakout_priority_full.txt

# Test breakout policy ***
python test_option.py --dataset-dir dummy --graph-dir data/breakout/breakout_priority_full --object Block --option-type model --sampler-type randblock --terminal-type inter --reward-type tscale --test-trials 10 --num-iters 300 --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --time-cutoff 300 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --breakout-variant breakout_priority_full --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 4000 --only-termination --use-interact --test-episode --param-contained --sum-rewards --drop-stopping --visualize-param noparam/hdd/datasets/counterfactual_data/breakout/videos/breakout_priority_full/ --save-interval 100 --save-raw --record-rollouts data/breakout/test_breakout_priority_full > logs/breakout/test_breakout_priority_full.txt


# Train breakout policy with hardened center with ball graph ***
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 2000 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 4000 --train --hidden-sizes 128 128 256 1024 128 --learning-type dqn --grad-epoch 50 --pretrain-iters 100000 --lr 1e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --breakout-variant center_full --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --test-episode --drop-stopping --max-critic 150 --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_center_full/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/center_full --save-graph data/breakout/center_full --save-interval 100 > logs/breakout/train_center_full.txt

# Test hardened center policy ***
python test_option.py --dataset-dir dummy --graph-dir data/breakout/center_full --object Block --option-type model --sampler-type randblock --terminal-type inter --reward-type tscale --test-trials 10 --num-iters 300 --reward-constant -1 --parameterized-lambda 1 --true-reward-lambda 1 --time-cutoff 300 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --breakout-variant center_full --observation-setting 1 0 0 0 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 3000 --only-termination --use-interact --test-episode --param-contained --sum-rewards --drop-stopping --visualize-param noparam/hdd/datasets/counterfactual_data/breakout/videos/center_full/ --save-interval 100 --save-raw --record-rollouts data/breakout/test_center_full > logs/breakout/test_center_full.txt


# Train single target block but multiple obstacle blocks ***
python train_option.py --dataset-dir dummy --graph-dir data/breakout/ball_graph --object Reward --option-type model --sampler-type randblock --policy-type pair --buffer-len 50000 --num-steps 500 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type inter --reward-type tscale --reward-constant -1 --parameterized-lambda 0 --true-reward-lambda 1 --epsilon-close .5 --time-cutoff 2000 --train --hidden-sizes 256 256 512 1024 128 --learning-type sac --grad-epoch 100 --pretrain-iters 100000 --lr 3e-5 --tau .001 --epsilon 0.1 --gpu 2 --log-interval 100 --interaction-probability 0 --max-steps 3000 --pretest-trials 20 --temporal-extend 300 --print-test  --interaction-prediction 0 --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --only-termination --use-interact --sum-rewards --prioritized-replay .2 .4 --param-contained --drop-stopping --terminate-cutoff --max-critic 100 --test-episode --save-pretrain /hdd/datasets/counterfactual_data/breakout/pretrain_harden_single/ --record-rollouts /hdd/datasets/counterfactual_data/breakout/live_record/harden_single --save-graph data/breakout/harden_single --save-interval 100 > logs/breakout/train_harden_single.txt


# Test single block obstacle policy ***
python test_option.py --dataset-dir dummy --graph-dir data/breakout/harden_single --object Block --option-type model --sampler-type randblock --terminal-type inter --reward-type tscale --test-trials 10 --num-iters 300 --reward-constant -1 --parameterized-lambda 10 --true-reward-lambda 2 --time-cutoff 300 --interaction-probability 0 --force-mask 0 0 1 1 0 --temporal-extend 300 --interaction-prediction 0 --use-termination --breakout-variant harden_single --observation-setting 1 0 0 1 0 0 0 0 0 --discretize-actions --env-reset --hardcode-norm breakout 4 1 --max-steps 2000 --only-termination --use-interact --test-episode --param-contained --sum-rewards --drop-stopping --test-episode --visualize-param noparam/hdd/datasets/counterfactual_data/breakout/videos/harden_single/ --save-interval 100 --save-raw --record-rollouts data/breakout/test_harden_single > logs/breakout/test_harden_single.txt


<!-- 
# 2DPushing domain
python generate_push_data.py data/pusher/pusher_random --num-frames 3000 --pushgripper

# train the gripper motion model
python construct_hypothesis_model.py --record-rollouts data/pusher/pusher_random/ --env 2DPushing --log-interval 500 --hidden-sizes 1024 1024 --predict-dynamics --action-shift --batch-size 10 --pretrain-iters 10000 --epsilon-schedule 3000 --num-iters 20000 --interaction-binary -1 -8 -16 --train --interaction-prediction .3 --base-variance 1e-4 --save-dir data/pusher/interaction_ag > logs/pusher/action_gripper_interaction.txt

# Run "test" to fill in parameter values for the hypothesis model
python construct_hypothesis_model.py --record-rollouts data/pusher/pusher_random --env 2DPushing --batch-size 64 --predict-dynamics --epsilon-schedule 1000 --action-shift --log-interval 1000 --num-frames 100000 --interaction-binary -1 -8 -16 --graph-dir data/action_graph/ --gpu 2  --dataset-dir data/pusher/interaction_ag --save-dir data/pusher/interaction_agv > logs/pusher/action_gripper_interactionv.txt

# train the gripper policy with HER and DQN
python train_option.py --dataset-dir data/pusher/interaction_agv/ --object Gripper --env 2DPushing --option-type model --buffer-len 500000 --num-steps 75 --gamma .99 --batch-size 16 --record-rollouts data/pusher/gripper --num-iters 4000 --terminal-type comb --reward-type comb --parameterized-lambda 30 --epsilon-close 1.5 --time-cutoff 75 --set-time-cutoff  --hidden-sizes 128 128 --learning-type herdqn --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --epsilon-schedule 200 --select-positive .5 --gpu 1 --resample-timer 20 --tau 2500 --log-interval 25 --reward-constant -1 --max-steps 30 --print-test --pretest-trials 20 --hardcode-norm robopush 1 1 --interaction-probability 0 --interaction-prediction 0 --save-graph data/pusher/gripper_graph --save-interval 100 > logs/pusher/train_gripper.txt

# test gripper policy
python test_option.py --dataset-dir data/pusher/interaction_agv/ --env 2DPushing --graph-dir data/pusher/gripper_graph --object Gripper --buffer-len 500000 --gamma .99 --record-rollouts data/pusher/gripper_test --test-trials 20 --num-iters 10 --terminal-type comb --reward-type comb --parameterized-lambda 0 --epsilon-close 1 --time-cutoff 50 --set-time-cutoff --gpu 1 --reward-constant -1 --max-steps 30 --print-test > logs/pusher/test_gripper.txt

# train block motion model
python construct_hypothesis_model.py --env 2DPushing --record-rollouts data/pusher/gripper --hidden-sizes 512 512 --batch-size 64 --pretrain-iters 10000 --interaction-iters 50000 --epsilon-schedule 1000 --num-iters 100000 --posttrain-iters 0 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --graph-dir data/pusher/gripper_graph/ --train --gpu 0 --interaction-prediction .3 --predict-dynamics --action-shift  --interaction-weight 100 --save-dir data/pusher/interaction_gbI > logs/pusher/gripper_block_interaction.txt

# Test block motion model
python construct_hypothesis_model.py --record-rollouts data/pusher/gripper --env 2DPushing --batch-size 64 --predict-dynamics --epsilon-schedule 1000 --action-shift --log-interval 1000 --num-frames 100000 --interaction-binary -1 -8 -16 --graph-dir data/pusher/gripper_graph/ --gpu 0  --dataset-dir data/pusher/interaction_gbI --save-dir data/pusher/interaction_gbIv > logs/pusher/gripper_block_interactionv.txt

python construct_hypothesis_model.py --record-rollouts data/pusher/gripper --env 2DPushing --batch-size 64 --log-interval 1000 --num-frames 100000 --interaction-binary -1 -13 -13 --graph-dir data/pusher/gripper_graph/ --gpu 2  --dataset-dir data/pusher/interaction_gbI
 -->
# robosuite pushing domain
# generate the data
python generate_random_robopushing.py 10000 /hdd/datasets/counterfactual_data/robopushing/live_record/random/ normalnoobs

# construct the forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/random/ --env RoboPushing --log-interval 500 --hidden-sizes 1024 1024 --predict-dynamics --action-shift --batch-size 64 --pretrain-iters 30000 --epsilon-schedule 3000 --num-iters 50000 --interaction-binary -1 -8 -16 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .001 --gpu 1 --save-dir data/robopushing/interaction_ag > logs/robopushing/action_gripper_interaction.txt

# test forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/random/ --env RoboPushing --batch-size 64 --predict-dynamics --epsilon-schedule 1000 --action-shift --log-interval 1000 --num-frames 100000 --interaction-binary -1 -8 -16 --norm-variance .025 --model-error-significance .5 --active-epsilon .01 --graph-dir data/robopushing/action_graph/ --gpu 2 --feature-step .25 --dataset-dir data/robopushing/interaction_ag --save-dir data/robopushing/interaction_agv > logs/robopushing/action_gripper_interactionv.txt

# Train the gripper policy with HER and SAC
<!-- python train_option.py --dataset-dir data/robopushing/interaction_agv/ --object Gripper --env RoboPushing --option-type model --buffer-len 500000 --num-steps 100 --gamma .99 --batch-size 16 --num-iters 1000 --terminal-type tcomb --reward-type comb --parameterized-lambda 1 --epsilon-close .05 --time-cutoff 50 --hidden-sizes 64 64 64 --learning-type hersac --grad-epoch 20 --pretrain-iters 1000 --lr 1e-4 --epsilon 0.5 --select-positive .5 --gpu 1 --resample-timer 50 --tau .005 --log-interval 50 --reward-constant -1 --max-steps 50 --param-recycle .1 --sample-continuous 2 --print-test --pretest-trials 2 --relative-param 1 --sac-alpha -1 --no-input --observation-setting 1 0 0 1 1 0 0 0 0 --test-trials 1 --early-stopping 1 --pretest-trials 20 --record-rollouts data/robopushing/gripper --save-graph data/robopushing/gripper_graph --save-interval 100 > logs/robopushing/train_gripper.txt
 -->
python train_option.py --dataset-dir data/robopushing/interaction_agv/ --object Gripper --env RoboPushing --option-type model --buffer-len 500000 --num-steps 90 --gamma .99 --batch-size 64 --record-rollouts data/robopushing/gripper --num-iters 4000 --terminal-type param --reward-type param --sampler-type cuuni --parameterized-lambda 1 --epsilon-close 0.004 --time-cutoff 30  --hidden-sizes 256 256 --learning-type herddpg --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2  --sample-schedule 5000 --sample-distance 0.3 --select-positive .5 --gpu 1 --resample-timer 30 --tau .001 --log-interval 25 --reward-constant -1 --max-steps 30 --print-test --pretest-trials 20 --hardcode-norm robopush 1 1 --observation-setting 1 0 0 1 1 0 0 0 0 --interaction-probability 0 --interaction-prediction 0 --use-termination --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/gripper/ --save-graph data/robopushing/gripper_graph --save-interval 100 > logs/robopushing/train_gripper.txt

# Train the Block forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/gripper/ --graph-dir data/robopushing/gripper_graph/ --env RoboPushing --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --pretrain-iters 10000 --epsilon-schedule 10000 --num-iters 50000 --interaction-binary -1 -1 -1 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .01 --train-pair Gripper Action Block --passive-weighting 10 --passive-error-cutoff .002 --hardcode-norm RoboPushing --no-pretrain-active --save-dir data/robopushing/interaction_gb > logs/robopushing/gripper_block_interaction.txt

# Test the Block forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/gripper/ --graph-dir data/robopushing/gripper_graph/ --env RoboPushing --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --interaction-binary -1 -1 -1 --interaction-prediction .3 --model-error-significance .01 --train-pair Gripper Action Block --hardcode-norm RoboPushing --active-epsilon .01 --feature-step .05 --dataset-dir data/robopushing/interaction_gb --save-dir data/robopushing/interaction_gbv > logs/robopushing/gripper_block_interactionv.txt

# Train the Block policy with HER and sac
python train_option.py --dataset-dir data/robopushing/interaction_gbv/ --graph-dir data/robopushing/gripper_graph --object Block --env RoboPushing --option-type model --buffer-len 1000000 --num-steps 200 --gamma .99 --batch-size 128 --num-iters 25000 --terminal-type param --reward-type param --sampler-type cuuni --sample-schedule 50000 --sample-distance .15 --parameterized-lambda 1 --epsilon-close 0.01 --hidden-sizes 128 128 128 128 --learning-type hersac --grad-epoch 75 --pretrain-iters 20000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2 --select-positive .3 --gpu 1 --tau .001 --log-interval 500 --reward-constant -1 --time-cutoff 200 --resample-timer 0 --max-steps 50 --print-test --pretest-trials 20 --hardcode-norm robopush 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --interaction-probability 0 --interaction-prediction 0 --her-only-interact 2 --relative-action .1 --temporal-extend 4 --prioritized-replay 0.2 0.4 --early-stopping 1 --use-termination --use-pair-gamma --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/block/ --save-graph data/robopushing/block_graph --save-interval 100 > logs/robopushing/train_block.txt

# Training Block policy with primitive actions
python train_option.py --dataset-dir data/robopushing/interaction_gbv/ --graph-dir data/robopushing/gripper_graph --object Block --env RoboPushing --option-type model --buffer-len 500000 --num-steps 150 --gamma .99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type param --sampler-type cuuni --sample-schedule 50000 --sample-distance .05 --parameterized-lambda 10 --epsilon-close 0.02 --time-cutoff 50 --hidden-sizes 128 128 64 --learning-type herddpg --grad-epoch 75 --pretrain-iters 10000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2 --epsilon-schedule 1000 --select-positive .5 --gpu 1 --resample-timer 0 --tau .001 --log-interval 500 --reward-constant -1 --max-steps 50 --print-test --pretest-trials 20 --hardcode-norm robopush 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --interaction-probability 0 --interaction-prediction 0 --her-only-interact 2 --early-stopping 5 --use-pair-gamma --primitive-actions --use-termination --record-rollouts data/robopushing/hyper/block_primitive --save-graph data/robopushing/hyper/block_primitive_graph --save-interval 500 > logs/robopushing/hyper/train_block_primitive.txt

# Testing Block policy
python test_option.py --dataset-dir data/robopushing/interaction_gbv/ --graph-dir data/robopushing/block_graph --object Block --env RoboPushing --option-type model --num-steps 150 --gamma .99 --batch-size 128 --num-iters 10 --test-trials 100 --terminal-type param --reward-type param --sampler-type cuuni --sample-distance .2 --parameterized-lambda 1 --epsilon-close 0.01 --hidden-sizes 128 128 64 --param-recycle .1 --sample-continuous 2 --gpu 1 --tau .001 --log-interval 500 --reward-constant -1 --time-cutoff 150 --max-steps 50 --print-test --hardcode-norm robopush 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --interaction-probability 0 --interaction-prediction 0 --her-only-interact 2 --relative-action .1 --temporal-extend 4 --early-stopping 1 --use-termination --use-pair-gamma --record-rollouts data/robopushing/test_block > logs/robopushing/test_block.txt


# Training the negative reward regions forward model


# Testing the negative reward regions forward model

# Training with the block policy and negative rewards
python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 15 --graph-dir data/robopushing/block_graphsd15 --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 600 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type negparam --parameterized-lambda 1 --true-reward-lambda -2 --reward-constant -.05 --epsilon-close .05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 200 --train --hidden-sizes 128 256 512 1024 128 --learning-type hersac --grad-epoch 300 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .2 --gpu 1 --resample-timer -1 --tau .001 --log-interval 50 --max-steps 200 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .07 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --interleave --terminate-reset --save-pretrain /hdd/datasets/counterfactual_data/robopushing/temp --record-rollouts /hdd/datasets/counterfactual_data/robopushing/live_record/reward --save-graph data/robopushing/reward_graph --save-interval 100 > logs/robopushing/train_reward.txt

# Testing block policy with negative rewards
python test_option.py --dataset-dir dummy --graph-dir data/robopushing/reward_graph --object Reward --env RoboPushing --num-obstacles 15 --option-type model --num-steps 150 --gamma .99 --num-iters 10 --test-trials 100 --terminal-type param --reward-type negparam --parameterized-lambda 1 --true-reward-lambda -2 --reward-constant -.05 --sampler-type tar --sample-distance .4 --epsilon-close 0.05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 150 --sample-continuous 2 --gpu 1 --log-interval 500 --time-cutoff 150 --max-steps 150 --print-test --hardcode-norm robopush 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .15 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --terminate-reset --interleave --visualize-param noparam/hdd/datasets/counterfactual_data/robopushing/videos/block_pushing/ --record-rollouts /hdd/counterfactual_data/robopushing/test_reward > logs/robopushing/test_reward.txt

# Testing graph search block policy
python test_option.py --dataset-dir data/robopushing/interaction_gbv/ --graph-dir data/robopushing/block_graph  --num-obstacles 10 --object Block --env RoboPushing --option-type model --num-steps 150 --gamma .99 --batch-size 128 --num-iters 30000 --terminal-type true --reward-type true --sampler-type path --sample-distance .4 --parameterized-lambda 1 --epsilon-close 0.01 --lr 1e-4 --gpu 1 --reward-constant -1 --time-cutoff 150 --max-steps 150 --print-test --hardcode-norm robopush 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --interaction-probability 0 --interaction-prediction 0 --relative-action .1 --temporal-extend 4 --early-stopping 1 --use-termination --use-pair-gamma --record-rollouts data/robopushing/block_search_test > logs/robopushing/test_search.txt

python test_option.py --dataset-dir data/robopushing/interaction_gbv/ --graph-dir data/robopushing/block_graph --object Block --env RoboPushing --option-type model --num-steps 150 --gamma .99 --batch-size 128 --num-iters 10 --test-trials 100 --terminal-type param --reward-type true --sampler-type cuuni --sample-distance .4 --parameterized-lambda 1 --epsilon-close 0.01 --hidden-sizes 128 128 64 --param-recycle .1 --sample-continuous 2 --gpu 1 --tau .001 --log-interval 500 --reward-constant 0 --time-cutoff 150 --max-steps 50 --print-test --hardcode-norm robopush 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --interaction-probability 0 --interaction-prediction 0 --her-only-interact 2 --relative-action .1 --temporal-extend 4 --early-stopping 1 --use-termination --use-pair-gamma --record-rollouts data/robopushing/test_block > logs/robopushing/test_block.txt

# robosuite planar pushing domain
# generate the data
python generate_random_robopushing.py 10000 /hdd/datasets/counterfactual_data/planar_robopushing/live_record/random/ planar

# construct the forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/planar_robopushing/live_record/random/ --env RoboPushing --planar-mode --log-interval 500 --hidden-sizes 1024 1024 --predict-dynamics --action-shift --batch-size 10 --pretrain-iters 10000 --epsilon-schedule 3000 --num-iters 50000 --interaction-binary -1 -1 -16 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .001 --gpu 1 --save-dir data/planar_robopushing/interaction_ag > logs/planar_robopushing/action_gripper_interaction.txt

# test forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/planar_robopushing/live_record/random/ --env RoboPushing --planar-mode --batch-size 64 --predict-dynamics --epsilon-schedule 1000 --action-shift --log-interval 1000 --num-frames 100000 --interaction-binary -1 -1 -16 --norm-variance .025 --model-error-significance .5 --active-epsilon .01 --graph-dir data/planar_robopushing/action_graph/ --gpu 2 --feature-step .25 --dataset-dir data/planar_robopushing/interaction_ag --save-dir data/planar_robopushing/interaction_agv > logs/planar_robopushing/action_gripper_interactionv.txt

# Train the gripper policy with HER and SAC
python train_option.py --dataset-dir data/planar_robopushing/interaction_agv/ --object Gripper --env RoboPushing --planar-mode --option-type model --buffer-len 500000 --num-steps 90 --gamma .99 --batch-size 64 --record-rollouts data/planar_robopushing/gripper --num-iters 4000 --terminal-type param --reward-type param --sampler-type cuuni --parameterized-lambda 1 --epsilon-close 0.004 --time-cutoff 30  --hidden-sizes 256 256 --learning-type herddpg --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2  --sample-schedule 5000 --sample-distance 0.3 --select-positive .5 --gpu 1 --resample-timer 30 --tau .001 --log-interval 25 --reward-constant -1 --max-steps 30 --print-test --pretest-trials 20 --hardcode-norm robopush 1 1 --observation-setting 1 0 0 1 1 0 0 0 0 --interaction-probability 0 --interaction-prediction 0 --use-termination --record-rollouts /hdd/datasets/counterfactual_data/planar_robopushing/live_record/gripper/ --save-graph data/planar_robopushing/gripper_graph --save-interval 100 > logs/planar_robopushing/train_gripper.txt

# Train the Block forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/planar_robopushing/live_record/gripper/ --graph-dir data/planar_robopushing/gripper_graph/ --env RoboPushing --planar-mode --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --pretrain-iters 10000 --epsilon-schedule 10000 --num-iters 50000 --interaction-binary -1 -1 -1 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .01 --train-pair Gripper Action Block --passive-weighting 10 --passive-error-cutoff .002 --hardcode-norm RoboPushing --no-pretrain-active --save-dir data/planar_robopushing/interaction_gb > logs/planar_robopushing/gripper_block_interaction.txt

# Test the Block forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/planar_robopushing/live_record/gripper/ --graph-dir data/planar_robopushing/gripper_graph/ --env RoboPushing --planar-mode --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --interaction-binary -1 -1 -1 --interaction-prediction .3 --model-error-significance .01 --train-pair Gripper Action Block --hardcode-norm RoboPushing --active-epsilon .01 --feature-step .05 --dataset-dir data/planar_robopushing/interaction_gb --save-dir data/planar_robopushing/interaction_gbv > logs/planar_robopushing/gripper_block_interactionv.txt

# Train the Block policy with HER and sac
python train_option.py --dataset-dir data/planar_robopushing/interaction_gbv/ --graph-dir data/planar_robopushing/gripper_graph --object Block --env RoboPushing --planar-mode --option-type model --buffer-len 1000000 --num-steps 200 --gamma .99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type param --sampler-type cuuni --sample-schedule 50000 --sample-distance .15 --parameterized-lambda 1 --epsilon-close 0.01 --hidden-sizes 128 128 128 128 --learning-type herddpg --grad-epoch 150 --pretrain-iters 20000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2 --select-positive .3 --gpu 1 --tau .001 --log-interval 500 --reward-constant -1 --time-cutoff 200 --resample-timer 0 --max-steps 50 --print-test --pretest-trials 20 --hardcode-norm robopush 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --interaction-probability 0 --interaction-prediction 0 --her-only-interact 2 --relative-action .1 --temporal-extend 4 --prioritized-replay 0.2 0.4 --early-stopping 1 --use-termination --use-pair-gamma --record-rollouts /hdd/datasets/counterfactual_data/planar_robopushing/live_record/block/ --save-graph data/planar_robopushing/block_graph --save-interval 100 > logs/planar_robopushing/train_block.txt

# Training with the block policy and negative rewards
python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 15 --graph-dir data/planar_robopushing/block_graph --planar-mode --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 600 --gamma 0.99 --batch-size 128 --num-iters 10000 --terminal-type param --reward-type negparam --parameterized-lambda 1 --true-reward-lambda -2 --reward-constant -.05 --epsilon-close .05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 200 --train --hidden-sizes 128 256 512 1024 128 --learning-type hersac --grad-epoch 300 --pretrain-iters 100000 --lr 1e-5 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .2 --gpu 1 --resample-timer -1 --tau .001 --log-interval 100 --max-steps 200 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .1 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --interleave --terminate-reset --save-pretrain /hdd/datasets/counterfactual_data/planar_robopushing/temp --record-rollouts /hdd/datasets/counterfactual_data/planar_robopushing/live_record/reward --save-graph data/planar_robopushing/reward_graph --save-interval 100 > logs/planar_robopushing/train_reward.txt

# Testing block policy with negative rewards
python test_option.py --dataset-dir dummy --graph-dir data/planar_robopushing/reward_graph --object Reward --env RoboPushing --planar-mode --num-obstacles 15 --option-type model --num-steps 150 --gamma .99 --num-iters 10 --test-trials 100 --terminal-type param --reward-type negparam --parameterized-lambda 1 --true-reward-lambda -2 --reward-constant -.05 --sampler-type tar --sample-distance .4 --epsilon-close 0.05 --param-norm 2 --negative-epsilon-close .03 --time-cutoff 150 --sample-continuous 2 --gpu 1 --log-interval 500 --time-cutoff 150 --max-steps 150 --print-test --hardcode-norm robopush 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --relative-action .15 --temporal-extend 20 --target Target --sum-reward --prioritized-replay 0.2 0.4 --param-contained --terminate-reset --interleave --visualize-param noparam/hdd/datasets/counterfactual_data/planar_robopushing/videos/block_pushing/ --record-rollouts /hdd/counterfactual_data/planar_robopushing/test_reward > logs/planar_robopushing/test_reward.txt


# robosuite hard obstacles domain
# Generate Random Data
python generate_random_robopushing.py 10000 /hdd/datasets/counterfactual_data/robopushingobst/live_record/random/ hard

# construct the hardobst gripper forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robopushingobst/live_record/random/ --env RoboPushing --log-interval 500 --hidden-sizes 128 256 512 --predict-dynamics --action-shift --batch-size 10 --pretrain-iters 10000 --epsilon-schedule 3000 --num-iters 50000 --interaction-binary -1 -8 -16 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .001 --train-pair Action Obstacle Gripper --gpu 1 --num-obstacles 10 --policy-type pair --instanced-additional --hardcode-norm RoboPushing --hard-obstacles --param-contained --save-dir data/robopushingobst/interaction_ag > logs/robopushingobst/action_gripper_interaction.txt

# test hardobst gripperforward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robopushingobst/live_record/random/ --env RoboPushing --batch-size 64 --predict-dynamics --epsilon-schedule 1000 --action-shift --log-interval 1000 --num-frames 100000 --interaction-binary -1 -8 -16 --norm-variance .025 --model-error-significance .5 --active-epsilon .01 --graph-dir data/robopushingobst/action_graph/ --gpu 2 --feature-step .25 --train-pair Action Obstacle Gripper --gpu 1 --num-obstacles 10 --policy-type pair --instanced-additional --hardcode-norm RoboPushing --hard-obstacles --param-contained --dataset-dir data/robopushingobst/interaction_ag --save-dir data/robopushingobst/interaction_agv > logs/robopushingobst/action_gripper_interactionv.txt

# Train the obstacle aware gripper policy with HER and SAC
python train_option.py --dataset-dir data/robopushingobst/interaction_agv/ --object Gripper --env RoboPushing --num-obstacles 10 --hard-obstacles --option-type model --policy-type pair --buffer-len 500000 --num-steps 90 --gamma .99 --batch-size 64 --record-rollouts data/robopushing/gripper --num-iters 4000 --terminal-type param --reward-type param --sampler-type cuuni --parameterized-lambda 1 --epsilon-close 0.007 --time-cutoff 30  --hidden-sizes 128 128 256 128 --learning-type herddpg --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2  --sample-schedule 5000 --sample-distance 0.3 --select-positive .5 --gpu 1 --resample-timer 30 --tau .001 --log-interval 25 --reward-constant -1 --max-steps 30 --print-test --pretest-trials 20 --hardcode-norm robopush 5 1 --observation-setting 1 0 0 1 1 0 0 0 0 --interaction-probability 0 --interaction-prediction 0 --use-termination --target-contained --param-contained --record-rollouts /hdd/datasets/counterfactual_data/robopushingobst/live_record/gripper/ --save-graph data/robopushingobst/gripper_graph --save-interval 100 > logs/robopushingobst/train_gripper.txt

# Train the Obstacle Block forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robopushingobst/live_record/gripper/ --graph-dir data/robopushingobst/gripper_graph/ --env RoboPushing --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --pretrain-iters 10000 --epsilon-schedule 10000 --num-iters 50000 --interaction-binary -1 -1 -1 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .01 --train-pair Gripper Action Obstacle Block --passive-weighting 10 --passive-error-cutoff .002 --num-obstacles 10 --policy-type pair --instanced-additional --hardcode-norm RoboPushing --hard-obstacles --param-contained --no-pretrain-active --save-dir data/robopushingobst/interaction_gb > logs/robopushingobst/gripper_block_interaction.txt

# Test the Obstacle Block forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robopushingobst/live_record/gripper/ --graph-dir data/robopushingobst/gripper_graph/ --env RoboPushing --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --interaction-binary -1 -1 -1 --interaction-prediction .3 --model-error-significance .01 --active-epsilon .01 --feature-step .05 --train-pair Gripper Action Obstacle Block --gpu 1 --num-obstacles 10 --policy-type pair --instanced-additional --hardcode-norm RoboPushing --hard-obstacles --param-contained --dataset-dir data/robopushingobst/interaction_gb --save-dir data/robopushingobst/interaction_gbv > logs/robopushingobst/gripper_block_interactionv.txt




# robosuite stick domain
# Generate Random Data
python generate_random_robostick.py 3000 /hdd/datasets/counterfactual_data/robostick/live_record/random

# train gripper forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robostick/live_record/random --env RoboStick --log-interval 500 --hidden-sizes 1024 1024 --predict-dynamics --action-shift --batch-size 10 --pretrain-iters 10000 --epsilon-schedule 3000 --num-iters 50000 --interaction-binary -1 -8 -16 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .001 --gpu 1 --save-dir data/robostick/interaction_ag > logs/robostick/action_gripper_interaction.txt

# Test gripper forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robostick/live_record/random/ --env RoboStick --batch-size 64 --predict-dynamics --epsilon-schedule 1000 --action-shift --log-interval 1000 --num-frames 100000 --interaction-binary -1 -8 -16 --norm-variance .025 --model-error-significance .5 --active-epsilon .01 --graph-dir data/action_graph/ --gpu 2 --feature-step .25 --dataset-dir data/robostick/interaction_ag --save-dir data/robostick/interaction_agv > logs/robostick/action_gripper_interactionv.txt

# Train Gripper policy
python train_option.py --dataset-dir data/robostick/interaction_agv/ --object Gripper --env RoboStick --option-type model --buffer-len 500000 --num-steps 100 --gamma .99 --batch-size 64 --num-iters 1000 --terminal-type param --reward-type param --sampler-type cuuni --parameterized-lambda 1 --epsilon-close 0.02 --time-cutoff 50  --hidden-sizes 128 128 --learning-type herddpg --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2  --sample-schedule 5000 --sample-distance 0.4 --select-positive .5 --gpu 1 --resample-timer 50 --tau .001 --log-interval 100 --reward-constant -1 --max-steps 30 --print-test --pretest-trials 20 --hardcode-norm robostick 1 1 --observation-setting 1 0 0 1 1 0 0 0 0 --interaction-probability 0 --interaction-prediction 0 --record-rollouts /hdd/datasets/counterfactual_data/robostick/live_record/gripper/ --save-graph data/robostick/gripper_graph --save-interval 100 > logs/robostick/train_gripper.txt

python train_option.py --dataset-dir data/robostick/interaction_agv/ --object Gripper --env RoboStick --option-type model --buffer-len 500000 --num-steps 100 --gamma .99 --batch-size 64 --num-iters 10000 --terminal-type param --reward-type param --sampler-type cuuni --parameterized-lambda 1 --epsilon-close 0.02 --time-cutoff 50  --hidden-sizes 128 128 --learning-type herddpg --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2  --sample-schedule 5000 --sample-distance 0.4 --select-positive .5 --gpu 1 --resample-timer 50 --tau .001 --log-interval 100 --reward-constant -1 --parameterized-lambda 1 --max-steps 30 --print-test --pretest-trials 20 --hardcode-norm robostick 1 1 --observation-setting 1 0 0 1 1 0 0 0 0 --interaction-probability 0 --interaction-prediction 0 --record-rollouts /hdd/datasets/counterfactual_data/robostick/live_record/gripper/ --save-graph data/robostick/gripper_graph --save-interval 100 > logs/robostick/train_gripper.txt


# Train Stick Forward model
python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robostick/live_record/gripper/ --graph-dir data/robostick/gripper_graph/ --env RoboStick --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --pretrain-iters 10000 --epsilon-schedule 10000 --interaction-iters 10000 --num-iters 50000 --interaction-binary -1 -1 -1 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .01 --train-pair Gripper Action Stick --passive-weighting 10 --passive-error-cutoff .002 --hardcode-norm RoboStick --no-pretrain-active

python construct_hypothesis_model.py --record-rollouts /hdd/datasets/counterfactual_data/robostick/live_record/gripper/ --graph-dir data/robostick/gripper_graph/ --env RoboStick --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --pretrain-iters 10000 --epsilon-schedule 10000 --num-iters 50000 --interaction-binary -1 -1 -1 --train --interaction-prediction .3 --norm-variance .025 --base-variance 1e-4 --model-error-significance .01 --train-pair Gripper Action Stick --passive-weighting 10 --passive-error-cutoff .002 --hardcode-norm RoboStick --no-pretrain-active --save-dir data/robostick/interaction_gs > logs/robostick/gripper_stick_interaction.txt

# Test Stick forward model
python construct_hypothesis_model.py --record-rollouts data/robostick/gripper/ --graph-dir data/robostick/gripper_graph/ --env RoboStick --log-interval 500 --hidden-sizes 512 512 --predict-dynamics --action-shift --batch-size 128 --interaction-binary -1 -1 -1 --interaction-prediction .3 --model-error-significance .01 --train-pair Gripper Action Stick --hardcode-norm RoboStick --active-epsilon .01 --feature-step .05 --dataset-dir data/robostick/interaction_gs --save-dir data/robostick/interaction_gsv > logs/robostick/gripper_stick_interactionv.txt

# Train stick policy with dummy model

python train_option.py --dataset-dir dummy --graph-dir data/robostick/gripper_graph --object Stick --env RoboStick --option-type model --buffer-len 1000000 --num-steps 200 --gamma .99 --batch-size 128 --num-iters 100000 --terminal-type param --reward-type comb --sampler-type cuuni --sample-schedule 50000 --sample-distance .4 --parameterized-lambda 1 --reward-constant -1 --interaction-lambda .5 --epsilon-close 0.01 --hidden-sizes 128 128 128 128 --learning-type herddpg --grad-epoch 75 --pretrain-iters 100000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2 --select-positive .2 --gpu 1 --tau .001 --log-interval 500 --time-cutoff 200 --resample-timer 0 --max-steps 50 --print-test --pretest-trials 20 --hardcode-norm robostick 3 1 --observation-setting 1 0 0 1 1 1 0 0 0 --interaction-probability 0 --interaction-prediction 0 --her-only-interact 2 --relative-action .1 --temporal-extend 5 --prioritized-replay 0.2 0.4 --early-stopping 1 --use-termination --use-pair-gamma --record-rollouts /hdd/datasets/counterfactual_data/robostick/live_record/stick --save-graph data/robopushing/stick_graph --save-interval 100 > logs/robostick/train_stick.txt


python train_option.py --dataset-dir data/robostick/interaction_agv/ --object Gripper --env RoboStick --option-type model --buffer-len 500000 --num-steps 100 --gamma .99 --batch-size 64 --num-iters 10000 --terminal-type param --reward-type param --sampler-type cuuni --parameterized-lambda 1 --epsilon-close 0.02 --time-cutoff 50  --hidden-sizes 128 128 --learning-type herddpg --grad-epoch 75 --pretrain-iters 1000 --lr 1e-4 --epsilon .1 --param-recycle .1 --sample-continuous 2  --sample-schedule 5000 --sample-distance 0.4 --select-positive .5 --gpu 1 --resample-timer 50 --tau .001 --log-interval 100 --reward-constant -1 --max-steps 30 --print-test --pretest-trials 20 --hardcode-norm robostick 1 1 --observation-setting 1 0 0 1 1 0 0 0 0 --interaction-probability 0 --interaction-prediction 0 --record-rollouts /hdd/datasets/counterfactual_data/robostick/live_record/gripper/ --save-graph data/robostick/gripper_graph --save-interval 100 > logs/robostick/train_gripper.txt


python train_option.py --dataset-dir dummy --env RoboPushing --num-obstacles 10 --graph-dir data/robopushing/block_graph --object Reward --option-type model --policy-type pair --buffer-len 500000 --num-steps 500 --gamma 0 --batch-size 128 --num-iters 10000 --terminal-type true --reward-type true --true-reward-lambda 1 --reward-constant 0 --epsilon-close .5 --time-cutoff 150 --train --hidden-sizes 128 128 128 128 128 --learning-type herddpg --grad-epoch 50 --pretrain-iters 100000 --lr 1e-4 --epsilon 0.1 --behavior-type greedyQ --sampler-type tar --select-positive .5 --gpu 1 --resample-timer -1 --tau .001 --log-interval 50 --max-steps 150 --print-test --pretest-trials 20 --hardcode-norm robopush 4 1 --interaction-probability 0 --interaction-prediction 0 --use-termination --observation-setting 1 0 0 1 1 0 0 0 0 --env-reset --relative-action .15 --temporal-extend 50 --target Target --sum-reward --prioritized-replay 0.2 0.4 --record-rollouts data/robopushing/reward --save-graph data/robopushing/reward_graph --save-interval 100 > logs/robopushing/train_reward.txt



single step action distribution
learn one step-reachability: predict if a state is reachable within some number of time steps. Output all of the reachable raw states
self-consistency: 